{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a dataframe with curated mutation counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify sites that are conserved in all clade founders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in clade founder sequences, add a column giving a site's sequence context, then make a list of sites where the site, its codon, and its 3mer sequence context are conserved across all founders and identical to the Wuhan-Hu-1 reference sequence (ignoring the codon requirement for nonidentical sites).\n",
    "\n",
    "**NOTE**: our GitHub repository does not track the directory defined by the variable `fitness_results_dir`. This directory contains results of cloning the [Bloom and Neher repository](https://github.com/jbloomlab/SARS2-mut-fitness) for estimating fitness effects and running the pipeline in that repository on an UShER tree with all sequences in GISAID as of 2024-04-24.\n",
    "\n",
    "However, we do track the files that this notebook generates, including the curated site-specific mutation counts in the file `results/curated_mut_counts.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "fitness_results_dir = '../SARS2-mut-fitness/results_gisaid_2024-04-24'\n",
    "founder_df = pd.read_csv(os.path.join(fitness_results_dir, 'clade_founder_nts/clade_founder_nts.csv'))\n",
    "founder_df.sort_values(['clade', 'site'], inplace=True)\n",
    "\n",
    "# Get founder seqs\n",
    "founder_seq_dict = {}\n",
    "for (clade, data) in founder_df.groupby('clade'):\n",
    "    founder_seq_dict[clade] = ''.join(data['nt'])\n",
    "\n",
    "# For each row, get the site's 3mer motif in the corresponding founder sequence\n",
    "def get_motif(site, clade):\n",
    "    founder_seq = founder_seq_dict[clade]\n",
    "    return founder_seq[site-2:site+1]\n",
    "min_and_max_sites = [founder_df['site'].min(), founder_df['site'].max()]\n",
    "founder_df['motif'] = founder_df.apply(\n",
    "    lambda row: np.nan if row['site'] in min_and_max_sites \\\n",
    "        else get_motif(row['site'], row['clade']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add columns giving the reference codon and motif\n",
    "founder_df = founder_df.merge(\n",
    "    (\n",
    "        founder_df[founder_df['clade'] == '19A']\n",
    "        .rename(columns={'codon' : 'ref_codon', 'motif' : 'ref_motif'})\n",
    "    )[['site', 'ref_codon', 'ref_motif']], on='site', how='left'\n",
    ")\n",
    "\n",
    "# Identify sites where the codon and motif are conserved across all clade founders\n",
    "# by subsetting data to entries with identical codons/motifs to reference, then\n",
    "# identifying sites that still have entries for all clades\n",
    "data = founder_df[\n",
    "    (founder_df['codon'] == founder_df['ref_codon']) &\n",
    "    (founder_df['motif'] == founder_df['ref_motif'])\n",
    "]\n",
    "site_counts = data['site'].value_counts()\n",
    "nclades = len(founder_df['clade'].unique())\n",
    "conserved_sites = site_counts[site_counts == nclades].index\n",
    "founder_df['same_context_all_founders'] = founder_df['site'].isin(conserved_sites)\n",
    "founder_df['nt_site'] = founder_df['site']\n",
    "\n",
    "print('Number of sites in genome:', len(founder_df['site'].unique()))\n",
    "print('Number of conserved sites:', len(conserved_sites))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and curate counts data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in dataframe on actual and expected counts, and add columns with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "counts_df = pd.read_csv(os.path.join(\n",
    "    fitness_results_dir,\n",
    "    'expected_vs_actual_mut_counts/expected_vs_actual_mut_counts.csv'\n",
    "))\n",
    "\n",
    "# Add metadata\n",
    "counts_df[['wt_nt', 'mut_nt']] = counts_df['nt_mutation'].str.extract(r'(\\w)\\d+(\\w)')\n",
    "counts_df['mut_type'] = counts_df['wt_nt'] + counts_df['mut_nt']\n",
    "\n",
    "def get_mut_class(row):\n",
    "    if row['synonymous']:\n",
    "        return 'synonymous'\n",
    "    elif row['noncoding']:\n",
    "        return 'noncoding'\n",
    "    elif '*' in row['mutant_aa']:\n",
    "        return 'nonsense'\n",
    "    elif row['mutant_aa'] != row['clade_founder_aa']:\n",
    "        return 'nonsynonymous'\n",
    "    else:\n",
    "        raise ValueError(row['mutant_aa'], row['clade_founder_aa'])\n",
    "\n",
    "counts_df['mut_class'] = counts_df.apply(lambda row: get_mut_class(row), axis=1)\n",
    "\n",
    "# Add column indicating if clade is pre-Omicron or Omicron\n",
    "pre_omicron_clades = [\n",
    "    '20A', '20B', '20C', '20E', '20G', '20H', '20I', '20J', '21C','21I', '21J'\n",
    "]\n",
    "counts_df['pre_omicron_or_omicron'] = counts_df['clade'].apply(\n",
    "    lambda x: 'pre_omicron' if x in pre_omicron_clades else 'omicron'\n",
    ")\n",
    "\n",
    "# Add column indicating if a site is before site 21,555\n",
    "counts_df['nt_site_before_21555'] = counts_df['nt_site'] < 21555\n",
    "\n",
    "# Add column indicating whether RNA sites from the Lan, 2022, Nature Comm. structure\n",
    "# are predicted to be paired, using code from Hensel, 2023, biorxiv\n",
    "filename = '../data/lan_2022/41467_2022_28603_MOESM11_ESM.txt'\n",
    "with open(filename) as f:\n",
    "    lines = [line.rstrip().split() for line in f]\n",
    "paired = np.array([[int(x[0]),int(x[4])] for x in lines[1:]])\n",
    "paired_dict = dict(zip(paired[:,0], paired[:,1]))\n",
    "def assign_ss_pred(site):\n",
    "    if site not in paired_dict:\n",
    "        return 'nd'\n",
    "    elif paired_dict[site] == 0:\n",
    "        return 'unpaired'\n",
    "    else:\n",
    "        return 'paired'\n",
    "counts_df['ss_prediction'] = counts_df['nt_site'].apply(lambda x: assign_ss_pred(x))\n",
    "\n",
    "# Add columns giving a site's motif relative to the clade founder\n",
    "# and the reference sequence\n",
    "counts_df = counts_df.merge(\n",
    "    founder_df[['nt_site', 'clade', 'motif', 'ref_motif']],\n",
    "    on = ['nt_site', 'clade'], how='left',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with curated counts. We curate the data in the following ways:\n",
    "* only analyze sites that pass the above conservation criteria\n",
    "* ignore sites that are annotated as being masked in any clade of the UShER tree (`masked_in_usher == True`), are annotated for exclusion (`exclude == True`), or were identified to highly homoplastic by De Maio et al. (https://virological.org/t/issues-with-sars-cov-2-sequencing-data/473)\n",
    "\n",
    "Then, subset the dataframe to one row for each possible mutation, including the following columns:\n",
    "* `actual_count`: gives the mutation's count for `subset == all` from the above dataframe of counts\n",
    "* additional columns give actual counts for subsets of the data, such as geographical subsets (England vs. USA) or phylogenetic subsets (pre-Omicron vs. Omicron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore sites that are masked or excluded in any clade of the UShER tree\n",
    "sites_to_ignore = list(counts_df[\n",
    "    (counts_df['masked_in_usher'] == True) |\n",
    "    (counts_df['exclude'] == True)\n",
    "]['nt_site'].unique())\n",
    "\n",
    "# Homoplastic sites from De Maio et al., which we will also ignore\n",
    "sites_to_ignore += [\n",
    "    187, 1059, 2094, 3037, 3130, 6990, 8022, 10323, 10741, 11074, 13408,\n",
    "    14786, 19684, 20148, 21137, 24034, 24378, 25563, 26144, 26461, 26681, 28077,\n",
    "    28826, 28854, 29700, 4050, 13402, 11083, 15324, 21575\n",
    "]\n",
    "\n",
    "# Aggregate counts across...\n",
    "# ... all clades for \"all\" subset\n",
    "ignore_cols = [\n",
    "    'expected_count', 'actual_count', 'count_terminal', 'count_non_terminal', 'mean_log_size',\n",
    "    'clade', 'pre_omicron_or_omicron'\n",
    "]\n",
    "groupby_cols = [\n",
    "    col for col in counts_df.columns.values\n",
    "    if col not in ignore_cols\n",
    "]\n",
    "curated_counts_df = counts_df[\n",
    "    (counts_df['nt_site'].isin(conserved_sites)) &\n",
    "    ~(counts_df['nt_site'].isin(sites_to_ignore)) &\n",
    "    (counts_df['subset'] == 'all')\n",
    "].groupby(groupby_cols, as_index=False).agg('sum', numeric_only=True)\n",
    "del curated_counts_df['mean_log_size']\n",
    "assert sum(curated_counts_df['nt_mutation'].duplicated(keep=False)) == 0\n",
    "\n",
    "# ... England or USA, and merge counts column with above dataframe\n",
    "subsets = ['England', 'USA']\n",
    "for subset in subsets:\n",
    "    subset_data = counts_df[\n",
    "        (counts_df['nt_site'].isin(conserved_sites)) &\n",
    "        ~(counts_df['nt_site'].isin(sites_to_ignore)) &\n",
    "        (counts_df['subset'] == subset)\n",
    "    ].groupby(groupby_cols, as_index=False).agg('sum', numeric_only=True)\n",
    "    assert sum(subset_data['nt_mutation'].duplicated(keep=False)) == 0\n",
    "    assert len(subset_data) == len(curated_counts_df)\n",
    "    curated_counts_df = curated_counts_df.merge(\n",
    "        (\n",
    "            subset_data\n",
    "            .rename(columns={'actual_count' : f'actual_count_{subset}'})\n",
    "        )[['nt_mutation', f'actual_count_{subset}']], on='nt_mutation'\n",
    "    )\n",
    "\n",
    "# ... pre-Omicron or Omicron clades, and merge counts column with above dataframe\n",
    "subsets = ['pre_omicron', 'omicron']\n",
    "for subset in subsets:\n",
    "    subset_data = counts_df[\n",
    "        (counts_df['nt_site'].isin(conserved_sites)) &\n",
    "        ~(counts_df['nt_site'].isin(sites_to_ignore)) &\n",
    "        (counts_df['subset'] == 'all') &\n",
    "        (counts_df['pre_omicron_or_omicron'] == subset)\n",
    "    ].groupby(groupby_cols, as_index=False).agg('sum', numeric_only=True)\n",
    "    assert sum(subset_data['nt_mutation'].duplicated(keep=False)) == 0\n",
    "    assert len(subset_data) == len(curated_counts_df)\n",
    "    curated_counts_df = curated_counts_df.merge(\n",
    "        (\n",
    "            subset_data\n",
    "            .rename(columns={'actual_count' : f'actual_count_{subset}'})\n",
    "        )[['nt_mutation', f'actual_count_{subset}']], on='nt_mutation'\n",
    "    )\n",
    "\n",
    "# Save curated counts to an output file\n",
    "assert sum(curated_counts_df['motif'] != curated_counts_df['ref_motif']) == 0\n",
    "assert len(curated_counts_df) == len(curated_counts_df['nt_mutation'].unique())\n",
    "curated_counts_df.drop(columns=['subset', 'exclude', 'masked_in_usher'], inplace=True)\n",
    "outfile = '../results/curated_mut_counts.csv'\n",
    "if not os.path.isfile(outfile):\n",
    "    curated_counts_df.to_csv(outfile, index=False)\n",
    "\n",
    "curated_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics of mutations in datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique muts:')\n",
    "print('In the full dataset:', len(counts_df['nt_mutation'].unique()))\n",
    "print('In the curated dataset:', len(curated_counts_df['nt_mutation'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique curated mutations per category:')\n",
    "curated_counts_df['mut_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of actual counts in the full dataset or specific subsets:')\n",
    "curated_counts_df[['actual_count', 'actual_count_pre_omicron', 'actual_count_omicron']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test splits using data on synonymous mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for synonymous mutations\n",
    "splits_df = curated_counts_df[curated_counts_df['synonymous'] == True].copy()\n",
    "\n",
    "# Determine the number of mutations to be included in training vs. testing data\n",
    "# using 80/20 splits\n",
    "ntotal = len(splits_df)\n",
    "ntrain = int(np.floor(ntotal * 0.8))\n",
    "ntest = ntotal - ntrain\n",
    "print('Total number of synonymous mutations in data:', ntotal)\n",
    "print('Number in each training set:', ntrain)\n",
    "print('Number in each test set:', ntest)\n",
    "\n",
    "# Generate 10 random train/test splits\n",
    "train_list = ['train'] * ntrain\n",
    "test_list = ['test'] * ntest\n",
    "split_list = train_list + test_list\n",
    "assert len(split_list) == ntotal\n",
    "nsplits = 10\n",
    "random.seed(1)\n",
    "for i in range(nsplits):\n",
    "    random.shuffle(split_list)\n",
    "    splits_df[f'split_{i}'] = split_list\n",
    "\n",
    "# Write dataframe to an output file\n",
    "outfile = '../results/syn_mut_train_test_splits.csv'\n",
    "if not os.path.isfile(outfile):\n",
    "    splits_df.to_csv(outfile, index=False)\n",
    "\n",
    "splits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the dataframe with counts at all sites to make a list of gene boundaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gene boundaires\n",
    "gene_boundaries_df = counts_df.groupby('gene', as_index=False).agg(\n",
    "    min_site = ('nt_site', 'min'),\n",
    "    max_site = ('nt_site', 'max'),\n",
    ")\n",
    "gene_boundaries_df['gene'].replace('ORF1a;ORF1ab', 'ORF1a', inplace=True)\n",
    "gene_boundaries_df['gene'].replace('ORF1ab', 'ORF1b', inplace=True)\n",
    "gene_boundaries_df = gene_boundaries_df[\n",
    "    ~(gene_boundaries_df['gene'].str.contains(';')) &\n",
    "    ~(gene_boundaries_df['gene'].isin(['noncoding']))\n",
    "].reset_index(drop=True).sort_values('min_site')\n",
    "\n",
    "# Save list to file\n",
    "outfile = '../results/gene_boundaries.csv'\n",
    "if not os.path.isfile(outfile):\n",
    "    gene_boundaries_df.to_csv(outfile, index=False)\n",
    "\n",
    "gene_boundaries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the total number of actual counts in a given subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    counts_df[\n",
    "        (counts_df['subset'] == 'all') &\n",
    "        ~(counts_df['nt_site'].isin(sites_to_ignore))\n",
    "    ]\n",
    "    .groupby(['pre_omicron_or_omicron'], as_index=False)['actual_count'].sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncov-ab-escape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
